{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8998 images belonging to 11 classes.\n",
      "Found 2247 images belonging to 11 classes.\n",
      "\n",
      "Class Labels: {'Call': 0, 'Done': 1, 'Hello': 2, 'Love': 3, 'No': 4, 'Peace': 5, 'Perfect': 6, 'Silence': 7, 'Thank you': 8, 'Water': 9, 'Yes': 10}\n",
      "Number of classes: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deekshith Patel L L\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deekshith Patel L L\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4536 - loss: 1.9158"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deekshith Patel L L\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.16288, saving model to gesture_model.keras\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m479s\u001b[0m 2s/step - accuracy: 0.4540 - loss: 1.9142 - val_accuracy: 0.1629 - val_loss: 10.1911 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7157 - loss: 0.9247\n",
      "Epoch 2: val_accuracy improved from 0.16288 to 0.24655, saving model to gesture_model.keras\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 2s/step - accuracy: 0.7158 - loss: 0.9244 - val_accuracy: 0.2466 - val_loss: 4.9632 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8175 - loss: 0.5962\n",
      "Epoch 3: val_accuracy did not improve from 0.24655\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m423s\u001b[0m 1s/step - accuracy: 0.8176 - loss: 0.5960 - val_accuracy: 0.2336 - val_loss: 5.2310 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8587 - loss: 0.4285\n",
      "Epoch 4: val_accuracy improved from 0.24655 to 0.35737, saving model to gesture_model.keras\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m485s\u001b[0m 2s/step - accuracy: 0.8587 - loss: 0.4285 - val_accuracy: 0.3574 - val_loss: 3.7266 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8559 - loss: 0.4230\n",
      "Epoch 5: val_accuracy improved from 0.35737 to 0.42546, saving model to gesture_model.keras\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 2s/step - accuracy: 0.8559 - loss: 0.4229 - val_accuracy: 0.4255 - val_loss: 3.1719 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8985 - loss: 0.3143\n",
      "Epoch 6: val_accuracy did not improve from 0.42546\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 2s/step - accuracy: 0.8985 - loss: 0.3142 - val_accuracy: 0.4166 - val_loss: 2.8347 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9116 - loss: 0.2789\n",
      "Epoch 7: val_accuracy improved from 0.42546 to 0.50111, saving model to gesture_model.keras\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 2s/step - accuracy: 0.9116 - loss: 0.2788 - val_accuracy: 0.5011 - val_loss: 2.6292 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9167 - loss: 0.2676\n",
      "Epoch 8: val_accuracy improved from 0.50111 to 0.65243, saving model to gesture_model.keras\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 2s/step - accuracy: 0.9167 - loss: 0.2675 - val_accuracy: 0.6524 - val_loss: 1.8068 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9240 - loss: 0.2237\n",
      "Epoch 9: val_accuracy did not improve from 0.65243\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m449s\u001b[0m 2s/step - accuracy: 0.9240 - loss: 0.2237 - val_accuracy: 0.4063 - val_loss: 4.8231 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9323 - loss: 0.2059\n",
      "Epoch 10: val_accuracy improved from 0.65243 to 0.71429, saving model to gesture_model.keras\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 2s/step - accuracy: 0.9323 - loss: 0.2059 - val_accuracy: 0.7143 - val_loss: 1.5678 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9288 - loss: 0.2201\n",
      "Epoch 11: val_accuracy did not improve from 0.71429\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m978s\u001b[0m 3s/step - accuracy: 0.9288 - loss: 0.2201 - val_accuracy: 0.5434 - val_loss: 2.2685 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9408 - loss: 0.1844\n",
      "Epoch 12: val_accuracy did not improve from 0.71429\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 2s/step - accuracy: 0.9408 - loss: 0.1844 - val_accuracy: 0.2483 - val_loss: 5.3381 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9468 - loss: 0.1598\n",
      "Epoch 13: val_accuracy did not improve from 0.71429\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 2s/step - accuracy: 0.9468 - loss: 0.1598 - val_accuracy: 0.3961 - val_loss: 4.4561 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9488 - loss: 0.1579\n",
      "Epoch 14: val_accuracy did not improve from 0.71429\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 2s/step - accuracy: 0.9488 - loss: 0.1578 - val_accuracy: 0.5594 - val_loss: 3.6313 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9498 - loss: 0.1483\n",
      "Epoch 15: val_accuracy did not improve from 0.71429\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 2s/step - accuracy: 0.9498 - loss: 0.1483 - val_accuracy: 0.6502 - val_loss: 1.7737 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9689 - loss: 0.1087\n",
      "Epoch 16: val_accuracy improved from 0.71429 to 0.77570, saving model to gesture_model.keras\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m855s\u001b[0m 3s/step - accuracy: 0.9689 - loss: 0.1087 - val_accuracy: 0.7757 - val_loss: 1.3972 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9695 - loss: 0.0970\n",
      "Epoch 17: val_accuracy improved from 0.77570 to 0.77926, saving model to gesture_model.keras\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m484s\u001b[0m 2s/step - accuracy: 0.9695 - loss: 0.0970 - val_accuracy: 0.7793 - val_loss: 1.2770 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9708 - loss: 0.0963\n",
      "Epoch 18: val_accuracy improved from 0.77926 to 0.78638, saving model to gesture_model.keras\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 2s/step - accuracy: 0.9708 - loss: 0.0963 - val_accuracy: 0.7864 - val_loss: 1.4125 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9639 - loss: 0.1045\n",
      "Epoch 19: val_accuracy did not improve from 0.78638\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 2s/step - accuracy: 0.9639 - loss: 0.1045 - val_accuracy: 0.7374 - val_loss: 1.6100 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9710 - loss: 0.0860\n",
      "Epoch 20: val_accuracy did not improve from 0.78638\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 2s/step - accuracy: 0.9710 - loss: 0.0860 - val_accuracy: 0.6680 - val_loss: 1.9994 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9712 - loss: 0.0887\n",
      "Epoch 21: val_accuracy improved from 0.78638 to 0.81531, saving model to gesture_model.keras\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 2s/step - accuracy: 0.9712 - loss: 0.0887 - val_accuracy: 0.8153 - val_loss: 1.2328 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9791 - loss: 0.0668\n",
      "Epoch 22: val_accuracy did not improve from 0.81531\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 2s/step - accuracy: 0.9791 - loss: 0.0669 - val_accuracy: 0.7664 - val_loss: 1.6929 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9672 - loss: 0.1030\n",
      "Epoch 23: val_accuracy did not improve from 0.81531\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 2s/step - accuracy: 0.9673 - loss: 0.1030 - val_accuracy: 0.8149 - val_loss: 1.0361 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9751 - loss: 0.0855\n",
      "Epoch 24: val_accuracy did not improve from 0.81531\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 2s/step - accuracy: 0.9751 - loss: 0.0855 - val_accuracy: 0.7873 - val_loss: 1.4365 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9786 - loss: 0.0730\n",
      "Epoch 25: val_accuracy improved from 0.81531 to 0.81709, saving model to gesture_model.keras\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 2s/step - accuracy: 0.9786 - loss: 0.0731 - val_accuracy: 0.8171 - val_loss: 1.2324 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9801 - loss: 0.0716\n",
      "Epoch 26: val_accuracy did not improve from 0.81709\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 2s/step - accuracy: 0.9801 - loss: 0.0716 - val_accuracy: 0.8073 - val_loss: 1.3654 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9787 - loss: 0.0655\n",
      "Epoch 27: val_accuracy did not improve from 0.81709\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m464s\u001b[0m 2s/step - accuracy: 0.9787 - loss: 0.0655 - val_accuracy: 0.7779 - val_loss: 1.3696 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9763 - loss: 0.0651\n",
      "Epoch 28: val_accuracy did not improve from 0.81709\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m450s\u001b[0m 2s/step - accuracy: 0.9763 - loss: 0.0651 - val_accuracy: 0.8122 - val_loss: 1.2677 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9778 - loss: 0.0665\n",
      "Epoch 29: val_accuracy improved from 0.81709 to 0.83178, saving model to gesture_model.keras\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 2s/step - accuracy: 0.9778 - loss: 0.0664 - val_accuracy: 0.8318 - val_loss: 1.1503 - learning_rate: 2.5000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9833 - loss: 0.0486\n",
      "Epoch 30: val_accuracy did not improve from 0.83178\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m454s\u001b[0m 2s/step - accuracy: 0.9833 - loss: 0.0486 - val_accuracy: 0.8251 - val_loss: 1.2085 - learning_rate: 2.5000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9851 - loss: 0.0474\n",
      "Epoch 31: val_accuracy did not improve from 0.83178\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m452s\u001b[0m 2s/step - accuracy: 0.9851 - loss: 0.0475 - val_accuracy: 0.8184 - val_loss: 1.3358 - learning_rate: 2.5000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9820 - loss: 0.0561\n",
      "Epoch 32: val_accuracy improved from 0.83178 to 0.83578, saving model to gesture_model.keras\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m491s\u001b[0m 2s/step - accuracy: 0.9820 - loss: 0.0561 - val_accuracy: 0.8358 - val_loss: 1.2916 - learning_rate: 2.5000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9885 - loss: 0.0387\n",
      "Epoch 33: val_accuracy did not improve from 0.83578\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 2s/step - accuracy: 0.9885 - loss: 0.0387 - val_accuracy: 0.8327 - val_loss: 1.2921 - learning_rate: 2.5000e-04\n",
      "Epoch 33: early stopping\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "\n",
      "Model saved to gesture_model.keras\n",
      "\n",
      "Final Training Accuracy: 0.9878\n",
      "Final Validation Accuracy: 0.8327\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Suppress TF logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Dataset Path\n",
    "DATASET_PATH = \"E:/miniproject/dataset/dataset/dataset\"\n",
    "MODEL_PATH = \"gesture_model.keras\"\n",
    "\n",
    "# Parameters\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "def create_model(num_classes):\n",
    "    model = Sequential([\n",
    "        # First Convolutional Block\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        # Second Convolutional Block\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        # Third Convolutional Block\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        # Dense Layers\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Data Augmentation\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    # Load Training Data\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        DATASET_PATH,\n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "\n",
    "    # Load Validation Data\n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "        DATASET_PATH,\n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "\n",
    "    # Print class indices\n",
    "    print(\"\\nClass Labels:\", train_generator.class_indices)\n",
    "    num_classes = len(train_generator.class_indices)\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "    # Create and compile model\n",
    "    model = create_model(num_classes)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            MODEL_PATH,\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Train the model\n",
    "    print(\"\\nStarting training...\")\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Save final model\n",
    "    model.save(MODEL_PATH)\n",
    "    print(f\"\\nModel saved to {MODEL_PATH}\")\n",
    "\n",
    "    # Print final metrics\n",
    "    final_accuracy = history.history['accuracy'][-1]\n",
    "    final_val_accuracy = history.history['val_accuracy'][-1]\n",
    "    print(f\"\\nFinal Training Accuracy: {final_accuracy:.4f}\")\n",
    "    print(f\"Final Validation Accuracy: {final_val_accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
